{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62301212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa88a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "PERIOD_SIZE = 50\n",
    "STEP_SIZE = 100\n",
    "MEDIAN_Q_THRESHOLD = 0.5\n",
    "LOWER_Q_THRESHOLD = 0.1\n",
    "UPPER_Q_THRESHOLD = 0.9\n",
    "ANOMALY_RES = [15, 11]  # Ground truth anomaly values\n",
    "RANDOM_SEEDS = [42, 123]  # For ensemble diversity\n",
    "LOOK_BACK = PERIOD_SIZE\n",
    "TW = STEP_SIZE\n",
    "MULTI = LOOK_BACK // TW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf26bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Swish Activation (CORRECTED)\n",
    "class Swish(Layer):\n",
    "    def __init__(self, beta=1.0, **kwargs):\n",
    "        super(Swish, self).__init__(**kwargs)\n",
    "        self.beta = K.cast_to_floatx(beta)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Correct Swish formula: x * sigmoid(beta * x)\n",
    "        return inputs * K.sigmoid(self.beta * inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'beta': float(self.beta)}\n",
    "        base_config = super(Swish, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "# %%\n",
    "# Utility Functions\n",
    "def verify_stationarity(dataset):\n",
    "    \"\"\"Check if time series is stationary using ADF test\"\"\"\n",
    "    is_stationary = True\n",
    "    test_results = adfuller(dataset)\n",
    "\n",
    "    print(f\"ADF test statistic: {test_results[0]:.4f}\")\n",
    "    print(f\"p-value: {test_results[1]:.4f}\")\n",
    "    print(\"Critical thresholds:\")\n",
    "\n",
    "    critical_value = None\n",
    "    for i, (key, value) in enumerate(test_results[4].items()):\n",
    "        print(f\"\\t{key}: {value:.3f}\")\n",
    "        if i == 0:\n",
    "            critical_value = value\n",
    "\n",
    "    if test_results[0] > critical_value:\n",
    "        print('Series is non-stationary')\n",
    "        is_stationary = False\n",
    "    else:\n",
    "        print('Series is stationary')\n",
    "    \n",
    "    return is_stationary\n",
    "\n",
    "\n",
    "def create_dataset(dataset, look_back=1, tw=3):\n",
    "    \"\"\"Create quantile-based features for training\"\"\"\n",
    "    dataX, dataY = [], []  # q50 (median)\n",
    "    dataUpperX, dataUpperY = [], []  # q90 (upper)\n",
    "    dataLowerX, dataLowerY = [], []  # q10 (lower)\n",
    "    multi = look_back // tw\n",
    "    \n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        q50X, q90X, q10X = [], [], []\n",
    "        a = dataset[i + 1:(i + look_back + 1)]\n",
    "        \n",
    "        # Calculate target quantiles\n",
    "        c = np.quantile(a, MEDIAN_Q_THRESHOLD)\n",
    "        u = np.quantile(a, UPPER_Q_THRESHOLD)\n",
    "        l = np.quantile(a, LOWER_Q_THRESHOLD)\n",
    "        \n",
    "        # Create sub-window features\n",
    "        for j in range(0, len(a), tw):\n",
    "            window = a[j:j + tw]\n",
    "            q50X.append(np.quantile(window, MEDIAN_Q_THRESHOLD))\n",
    "            q90X.append(np.quantile(window, UPPER_Q_THRESHOLD))\n",
    "            q10X.append(np.quantile(window, LOWER_Q_THRESHOLD))\n",
    "        \n",
    "        dataX.append(q50X)\n",
    "        dataY.append(c)\n",
    "        dataUpperX.append(q90X)\n",
    "        dataUpperY.append(u)\n",
    "        dataLowerX.append(q10X)\n",
    "        dataLowerY.append(l)\n",
    "    \n",
    "    return (np.array(dataX), np.array(dataY), \n",
    "            np.array(dataUpperX), np.array(dataUpperY), \n",
    "            np.array(dataLowerX), np.array(dataLowerY))\n",
    "\n",
    "\n",
    "def quantile_loss(q):\n",
    "    \"\"\"Proper quantile loss function for training\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        e = y_true - y_pred\n",
    "        return K.mean(K.maximum(q * e, (q - 1) * e))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def create_quantile_model(quantile, input_shape, seed=42, units=64):\n",
    "    \"\"\"Create improved LSTM model with proper architecture\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(units, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units // 2),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1),\n",
    "        Swish(beta=1.5)  # Single activation layer\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss=quantile_loss(quantile),\n",
    "        optimizer='adam',\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_ensemble(models, input_data):\n",
    "    \"\"\"Average predictions from multiple models\"\"\"\n",
    "    predictions = [model.predict(input_data, verbose=0) for model in models]\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "\n",
    "def identify_alpha(dataset):\n",
    "    \"\"\"Calculate trend stability metric\"\"\"\n",
    "    alpha_detection = []\n",
    "    prev_slope = 1\n",
    "    \n",
    "    for m in range(0, len(dataset), PERIOD_SIZE):\n",
    "        period_dataset = dataset[m:m + PERIOD_SIZE]\n",
    "        if len(period_dataset) < 2:\n",
    "            continue\n",
    "        # Extract scalar values from arrays\n",
    "        start_val = float(period_dataset[0][0]) if period_dataset[0].ndim > 0 else float(period_dataset[0])\n",
    "        end_val = float(period_dataset[-1][0]) if period_dataset[-1].ndim > 0 else float(period_dataset[-1])\n",
    "        slope = (end_val - start_val) / len(period_dataset)\n",
    "        alpha = slope / prev_slope if prev_slope != 0 else 1\n",
    "        alpha_detection.append(float(alpha))\n",
    "        prev_slope = slope if slope != 0 else 1\n",
    "    \n",
    "    return float(np.abs(np.mean(alpha_detection))) if alpha_detection else 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9cb34",
   "metadata": {},
   "source": [
    "LOADING AND PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a69f4f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shape: (1745, 1)\n",
      "Dataset range: [17.00, 77.00]\n",
      "\n",
      "Stationarity Test:\n",
      "ADF test statistic: -22.0681\n",
      "p-value: 0.0000\n",
      "Critical thresholds:\n",
      "\t1%: -3.434\n",
      "\t5%: -2.863\n",
      "\t10%: -2.568\n",
      "Series is stationary\n",
      "\n",
      "Train size: 1047\n",
      "Validation size: 349\n",
      "Test size: 349\n",
      "\n",
      "Alpha (trend stability): 1.1422\n",
      "\n",
      "============================================================\n",
      "CREATING QUANTILE DATASETS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(7)\n",
    "dataframe = read_csv('Q-TV-RNN/Q-data/speed_t4013_train.csv', usecols=[2], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# Check for NaN/Inf\n",
    "assert not np.isnan(dataset).any(), \"Dataset contains NaN values\"\n",
    "assert not np.isinf(dataset).any(), \"Dataset contains Inf values\"\n",
    "\n",
    "print(f\"\\nDataset shape: {dataset.shape}\")\n",
    "print(f\"Dataset range: [{dataset.min():.2f}, {dataset.max():.2f}]\")\n",
    "\n",
    "# Verify stationarity\n",
    "print(\"\\nStationarity Test:\")\n",
    "stationary = verify_stationarity(dataset)\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# Split into train/validation/test sets (60/20/20)\n",
    "train_size = int(len(dataset) * 0.6)\n",
    "val_size = int(len(dataset) * 0.2)\n",
    "train = dataset[0:train_size, :]\n",
    "val = dataset[train_size:train_size + val_size, :]\n",
    "test = dataset[train_size + val_size:, :]\n",
    "\n",
    "print(f\"\\nTrain size: {len(train)}\")\n",
    "print(f\"Validation size: {len(val)}\")\n",
    "print(f\"Test size: {len(test)}\")\n",
    "\n",
    "# Calculate alpha (trend metric)\n",
    "alpha = identify_alpha(dataset)\n",
    "print(f\"\\nAlpha (trend stability): {alpha:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Create datasets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING QUANTILE DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainX, trainY, trainXU, trainYU, trainXL, trainYL = create_dataset(train, LOOK_BACK, TW)\n",
    "valX, valY, valXU, valYU, valXL, valYL = create_dataset(val, LOOK_BACK, TW)\n",
    "testX, testY, testXU, testYU, testXL, testYL = create_dataset(test, LOOK_BACK, TW)\n",
    "\n",
    "# Reshape for LSTM [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "valX = np.reshape(valX, (valX.shape[0], valX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "trainXU = np.reshape(trainXU, (trainXU.shape[0], trainXU.shape[1], 1))\n",
    "valXU = np.reshape(valXU, (valXU.shape[0], valXU.shape[1], 1))\n",
    "testXU = np.reshape(testXU, (testXU.shape[0], testXU.shape[1], 1))\n",
    "\n",
    "trainXL = np.reshape(trainXL, (trainXL.shape[0], trainXL.shape[1], 1))\n",
    "valXL = np.reshape(valXL, (valXL.shape[0], valXL.shape[1], 1))\n",
    "testXL = np.reshape(testXL, (testXL.shape[0], testXL.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47387d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (MULTI, 1)\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, restore_best_weights=True, verbose=1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc06638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1/3 (seed=42):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"lstm_4\" (type LSTM).\n\nslice index 0 of dimension 0 out of bounds. for '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](transpose, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)' with input shapes: [0,?,1], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 0, 1), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(RANDOM_SEEDS):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/3 (seed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_quantile_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMEDIAN_Q_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      6\u001b[0m         trainX, trainY,\n\u001b[0;32m      7\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m(valX, valY),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     models_q50\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "Cell \u001b[1;32mIn[10], line 93\u001b[0m, in \u001b[0;36mcreate_quantile_model\u001b[1;34m(quantile, input_shape, seed, units)\u001b[0m\n\u001b[0;32m     90\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m     91\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(seed)\n\u001b[1;32m---> 93\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSwish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Single activation layer\u001b[39;49;00m\n\u001b[0;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    104\u001b[0m     loss\u001b[38;5;241m=\u001b[39mquantile_loss(quantile),\n\u001b[0;32m    105\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    106\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\suraj\\anaconda3\\envs\\tf2.12\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:629\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\suraj\\anaconda3\\envs\\tf2.12\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\suraj\\anaconda3\\envs\\tf2.12\\lib\\site-packages\\keras\\backend.py:4595\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   4586\u001b[0m input_ta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m   4587\u001b[0m     ta\u001b[38;5;241m.\u001b[39munstack(input_) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m go_backwards \u001b[38;5;28;01melse\u001b[39;00m ta\n\u001b[0;32m   4588\u001b[0m     \u001b[38;5;241m.\u001b[39munstack(reverse(input_, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m   4589\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ta, input_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_ta, flatted_inputs))\n\u001b[0;32m   4591\u001b[0m \u001b[38;5;66;03m# Get the time(0) input and compute the output for that, the output will be\u001b[39;00m\n\u001b[0;32m   4592\u001b[0m \u001b[38;5;66;03m# used to determine the dtype of output tensor array. Don't read from\u001b[39;00m\n\u001b[0;32m   4593\u001b[0m \u001b[38;5;66;03m# input_ta due to TensorArray clear_after_read default to True.\u001b[39;00m\n\u001b[0;32m   4594\u001b[0m input_time_zero \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(inputs,\n\u001b[1;32m-> 4595\u001b[0m                                         [\u001b[43minp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m flatted_inputs])\n\u001b[0;32m   4596\u001b[0m \u001b[38;5;66;03m# output_time_zero is used to determine the cell output shape and its dtype.\u001b[39;00m\n\u001b[0;32m   4597\u001b[0m \u001b[38;5;66;03m# the value is discarded.\u001b[39;00m\n\u001b[0;32m   4598\u001b[0m output_time_zero, _ \u001b[38;5;241m=\u001b[39m step_function(\n\u001b[0;32m   4599\u001b[0m     input_time_zero, \u001b[38;5;28mtuple\u001b[39m(initial_states) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(constants))\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"lstm_4\" (type LSTM).\n\nslice index 0 of dimension 0 out of bounds. for '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](transpose, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)' with input shapes: [0,?,1], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 0, 1), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "models_q50 = []\n",
    "for i, seed in enumerate(RANDOM_SEEDS):\n",
    "    print(f\"\\nModel {i+1}/3 (seed={seed}):\")\n",
    "    model = create_quantile_model(MEDIAN_Q_THRESHOLD, input_shape, seed)\n",
    "    history = model.fit(\n",
    "        trainX, trainY,\n",
    "        validation_data=(valX, valY),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    models_q50.append(model)\n",
    "    print(f\"  Final train loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"  Final val loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "# Train Q10 (lower) models\n",
    "print(\"\\n--- Training Q10 (Lower Bound) Models ---\")\n",
    "models_q10 = []\n",
    "for i, seed in enumerate(RANDOM_SEEDS):\n",
    "    print(f\"\\nModel {i+1}/3 (seed={seed}):\")\n",
    "    model = create_quantile_model(LOWER_Q_THRESHOLD, input_shape, seed)\n",
    "    history = model.fit(\n",
    "        trainXL, trainYL,\n",
    "        validation_data=(valXL, valYL),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    models_q10.append(model)\n",
    "    print(f\"  Final train loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"  Final val loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "# Train Q90 (upper) models\n",
    "print(\"\\n--- Training Q90 (Upper Bound) Models ---\")\n",
    "models_q90 = []\n",
    "for i, seed in enumerate(RANDOM_SEEDS):\n",
    "    print(f\"\\nModel {i+1}/3 (seed={seed}):\")\n",
    "    model = create_quantile_model(UPPER_Q_THRESHOLD, input_shape, seed)\n",
    "    history = model.fit(\n",
    "        trainXU, trainYU,\n",
    "        validation_data=(valXU, valYU),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    models_q90.append(model)\n",
    "    print(f\"  Final train loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"  Final val loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "print(\"\\n✓ All ensemble models trained successfully!\")\n",
    "\n",
    "# %%\n",
    "# Determine optimal threshold from validation set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LEARNING OPTIMAL THRESHOLD FROM VALIDATION SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "val_predictions_q50 = predict_ensemble(models_q50, valX)\n",
    "val_predictions_q10 = predict_ensemble(models_q10, valXL)\n",
    "val_predictions_q90 = predict_ensemble(models_q90, valXU)\n",
    "\n",
    "val_errors = []\n",
    "for i in range(len(valX)):\n",
    "    actual = val[LOOK_BACK + i + 1]\n",
    "    iqr = val_predictions_q90[i] - val_predictions_q10[i]\n",
    "    error = abs(actual - val_predictions_q50[i]) / (iqr + 1e-6)\n",
    "    val_errors.append(error[0])\n",
    "\n",
    "# Use 95th percentile of validation errors as threshold\n",
    "threshold_multiplier = np.quantile(val_errors, 0.95)\n",
    "print(f\"Learned threshold multiplier: {threshold_multiplier:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623087bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
