{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee03bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from numpy import *\n",
    "import decoding \n",
    "import analysis\n",
    "import training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762eff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "period_size = 150\n",
    "step_size = 50\n",
    "median_q_threshold=0.5\n",
    "lower_q_threshold=0.1\n",
    "upper_q_threshold=0.9\n",
    "anomaly_res =[15,11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Swish(Layer):\n",
    "    def __init__(self, beta, **kwargs):\n",
    "        super(Swish, self).__init__(**kwargs)\n",
    "        self.beta = K.cast_to_floatx(beta)\n",
    "        print('beta',self.beta)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print('beta', self.beta,'input',input)\n",
    "        return K.softsign(inputs) * self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'beta': float(self.beta)}\n",
    "        base_config = super(Swish, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3=[]\n",
    "    for itr in range(len(lst1)):\n",
    "        if lst1[itr][0] in lst2:\n",
    "            lst3.append(lst1[itr])\n",
    "    return lst3\n",
    "\n",
    "\n",
    "\n",
    "def verify_stationarity(dataset):\n",
    "    is_stationary=True\n",
    "    test_results = adfuller(dataset)\n",
    "\n",
    "    print(f\"ADF test statistic: {test_results[0]}\")\n",
    "    print(f\"p-value: {test_results[1]}\")\n",
    "    print(\"Critical thresholds:\")\n",
    "\n",
    "    for key, value in test_results[4].items():\n",
    "        print(f\"\\t{key}: {value}\")\n",
    "    itr = 0\n",
    "    for key, value in test_results[4].items():\n",
    "       print('\\t%s: %.3f' % (key, value))\n",
    "       if itr==0:\n",
    "         critical=value\n",
    "       itr=itr+1\n",
    "\n",
    "    print('critical',critical)\n",
    "    if test_results[0] > critical:\n",
    "         print('non stationary')\n",
    "         is_stationary=False\n",
    "    return  is_stationary\n",
    "\n",
    "def create_dataset(dataset, look_back=1, tw=3):\n",
    "    dataX, dataY = [], []  # dtaset for mean\n",
    "    dataUpperX, dataUpperY = [], []  # dataset for std\n",
    "    dataLowerX, dataLowerY = [], []  # dataset for mean and std for third deep learning\n",
    "    multi = look_back // tw\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        q50X = []\n",
    "        q90X =  []\n",
    "        q10X = []\n",
    "        a = dataset[i + 1:(i + look_back + 1)]\n",
    "        indices = i + (multi - 1) * tw\n",
    "        # print('last window', dataset[indices:(i + look_back), 0])\n",
    "        c = numpy.quantile(a, median_q_threshold)\n",
    "        u = numpy.quantile(a, upper_q_threshold)\n",
    "        l = numpy.quantile(a, lower_q_threshold)\n",
    "        for j in range(0, len(a), tw):\n",
    "            q50 = numpy.quantile(a[j:j + tw], median_q_threshold)\n",
    "            q90 = numpy.quantile(a[j:j + tw], upper_q_threshold)\n",
    "            q10 = numpy.quantile(a[j:j + tw], lower_q_threshold)\n",
    "            q50X.append(q50)\n",
    "            q90X.append(q90)\n",
    "            q10X.append(q10)\n",
    "        dataX.append(q50X)\n",
    "        dataY.append(c)\n",
    "        dataUpperX.append(q90X)\n",
    "        dataUpperY.append(u)\n",
    "        dataLowerX.append(q10X)\n",
    "        dataLowerY.append(l)\n",
    "\n",
    "    return numpy.array(dataX), numpy.array(dataY), numpy.array(dataUpperX), numpy.array(dataUpperY), numpy.array(dataLowerX), numpy.array(dataLowerY)\n",
    "def identify_anomaly_quantiles(prediction_errors):\n",
    "    anomaly_detection=[]\n",
    "    for m in range(0, len(prediction_errors), period_size):\n",
    "        period_prediction_errors=prediction_errors[m:m + period_size]\n",
    "        upper_threshold = numpy.quantile(prediction_errors[m:m + period_size],0.9)\n",
    "        lower_threshold = numpy.quantile(prediction_errors[m:m + period_size],0.1)\n",
    "        #upper_threshold=avg+2*std1\n",
    "        #lower_threshold = avg - 2 * std1\n",
    "        for i in range(len(period_prediction_errors)):\n",
    "            if (period_prediction_errors[i]>0 and period_prediction_errors[i]> upper_threshold) or (period_prediction_errors[i]<0 and period_prediction_errors[i]< lower_threshold):\n",
    "                anomaly_detection.append(period_prediction_errors[i])\n",
    "\n",
    "    return anomaly_detection\n",
    "\n",
    "def identify_anomaly(prediction_errors):\n",
    "    anomaly_detection=[]\n",
    "    for m in range(0, len(prediction_errors), period_size):\n",
    "        period_prediction_errors=prediction_errors[m:m + period_size]\n",
    "        avg = numpy.average(prediction_errors[m:m + period_size])\n",
    "        std1 = numpy.std(prediction_errors[m:m + period_size])\n",
    "        upper_threshold=avg+2*std1\n",
    "        lower_threshold = avg - 2 * std1\n",
    "        for i in range(len(period_prediction_errors)):\n",
    "            if (period_prediction_errors[i]> upper_threshold) or ( period_prediction_errors[i]< lower_threshold):\n",
    "                anomaly_detection.append(period_prediction_errors[i])\n",
    "\n",
    "\n",
    "    return  anomaly_detection\n",
    "\n",
    "def identify_alpha(dataset):\n",
    "    alpha_detection=[]\n",
    "    alpha=1\n",
    "    prev_slope=1\n",
    "    for m in range(0, len(dataset), period_size):\n",
    "        period_dataset=dataset[m:m + period_size]\n",
    "        slope=period_dataset[len(period_dataset)-1]-period_dataset[0]\n",
    "        slope=slope/period_size\n",
    "        alpha=slope/prev_slope\n",
    "        alpha_detection.append(alpha)\n",
    "        prev_slope=slope\n",
    "\n",
    "    return  numpy.absolute(numpy.mean(alpha_detection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('/content/speed_t4013_train.csv', usecols=[2], engine='python')\n",
    "dataset = dataframe.values\n",
    "stationary=verify_stationarity(dataset)\n",
    "#dataframe=dataframe.diff(axis = 0, periods = 1)\n",
    "#dataset = dataframe.dropna().values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "print('dataset', dataset)\n",
    "#dataset=dataset.dropna()\n",
    "stationary = verify_stationarity(dataset)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "std=numpy.std(train)\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = period_size\n",
    "tw = step_size\n",
    "multi = look_back // tw\n",
    "trainX, trainY, trainXU, trainYU, trainXL, trainYL = create_dataset(train, look_back, tw)\n",
    "testX, testY , testXU, testYU, testXL, testYL= create_dataset(test, look_back, tw)\n",
    "print(trainX)\n",
    "plt.figure(figsize=(12, 6))\n",
    "# plt.plot(ts, label=\"passengers\")\n",
    "plt.plot(train, label=\"dataset\")\n",
    "# plt.plot(anomalies_array, label=\"difference\")\n",
    "# plt.plot(newdata['ewa'], label=\"ewa\")\n",
    "plt.legend(loc='best')\n",
    "\n",
    "#plt.show()\n",
    "alpha=identify_alpha(dataset)\n",
    "print('alpha',alpha)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "trainXU = numpy.reshape(trainXU, (trainXU.shape[0], trainXU.shape[1], 1))\n",
    "testXU = numpy.reshape(testXU, (testXU.shape[0], testXU.shape[1], 1))\n",
    "\n",
    "trainXL = numpy.reshape(trainXL, (trainXL.shape[0], trainXL.shape[1], 1))\n",
    "testXL = numpy.reshape(testXL, (testXL.shape[0], testXL.shape[1], 1))\n",
    "print(trainX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4894eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting datasets into 3 parts for ensemble training...\")\n",
    "\n",
    "# Split q50 (median) data\n",
    "trainX_parts = numpy.array_split(trainX, 3)\n",
    "trainY_parts = numpy.array_split(trainY, 3)\n",
    "\n",
    "# Split q10 (lower) data\n",
    "trainXL_parts = numpy.array_split(trainXL, 3)\n",
    "trainYL_parts = numpy.array_split(trainYL, 3)\n",
    "\n",
    "# Split q90 (upper) data\n",
    "trainXU_parts = numpy.array_split(trainXU, 3)\n",
    "trainYU_parts = numpy.array_split(trainYU, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db11ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_shape = (multi, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a07827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train 3 models for q50 (Median) ---\n",
    "print(\"Training q50 ensemble...\")\n",
    "model_q50_1 = build_rnn_model(model_input_shape)\n",
    "model_q50_2 = build_rnn_model(model_input_shape)\n",
    "model_q50_3 = build_rnn_model(model_input_shape)\n",
    "\n",
    "model_q50_1.fit(trainX_parts[0], trainY_parts[0], epochs=50, batch_size=1, verbose=2)\n",
    "model_q50_2.fit(trainX_parts[1], trainY_parts[1], epochs=50, batch_size=1, verbose=2)\n",
    "model_q50_3.fit(trainX_parts[2], trainY_parts[2], epochs=50, batch_size=1, verbose=2)\n",
    "\n",
    "# --- Train 3 models for q10 (Lower) ---\n",
    "print(\"Training q10 ensemble...\")\n",
    "model_q10_1 = build_rnn_model(model_input_shape)\n",
    "model_q10_2 = build_rnn_model(model_input_shape)\n",
    "model_q10_3 = build_rnn_model(model_input_shape)\n",
    "\n",
    "model_q10_1.fit(trainXL_parts[0], trainYL_parts[0], epochs=50, batch_size=1, verbose=2)\n",
    "model_q10_2.fit(trainXL_parts[1], trainYL_parts[1], epochs=50, batch_size=1, verbose=2)\n",
    "model_q10_3.fit(trainXL_parts[2], trainYL_parts[2], epochs=50, batch_size=1, verbose=2)\n",
    "\n",
    "# --- Train 3 models for q90 (Upper) ---\n",
    "print(\"Training q90 ensemble...\")\n",
    "model_q90_1 = build_rnn_model(model_input_shape)\n",
    "model_q90_2 = build_rnn_model(model_input_shape)\n",
    "model_q90_3 = build_rnn_model(model_input_shape)\n",
    "\n",
    "model_q90_1.fit(trainXU_parts[0], trainYU_parts[0], epochs=50, batch_size=1, verbose=2)\n",
    "model_q90_2.fit(trainXU_parts[1], trainYU_parts[1], epochs=50, batch_size=1, verbose=2)\n",
    "model_q90_3.fit(trainXU_parts[2], trainYU_parts[2], epochs=50, batch_size=1, verbose=2)\n",
    "\n",
    "print(\"--- 9-Model Ensemble Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5c5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc4c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainmeanPredict = modelmean.predict(trainX)\n",
    "# testmeanPredict = modelmean.predict(testX)\n",
    "# trainstdPredict = modelstd.predict(trainX)\n",
    "# teststdPredict = modelstd.predict(testX)\n",
    "# print('trainmeanPredict',trainmeanPredict)\n",
    "i = 0\n",
    "j = look_back\n",
    "actual_quantile_interval = []\n",
    "steps = tw\n",
    "positive = True\n",
    "anomalies=[]\n",
    "finalres_q10 = []\n",
    "finalres_q90 = []\n",
    "dataframe = read_csv('/content/speed_t4013_labelled.csv', usecols=[2], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "ts = dataset\n",
    "ts_accumulate=[]\n",
    "comparison_dataset=[]\n",
    "while j <= len(dataset):\n",
    "    q50_array = []\n",
    "    q10_array = []\n",
    "    q90_array = []\n",
    "\n",
    "\n",
    "    temp = dataset[i:j]\n",
    "    actual_quantile_interval.append(\n",
    "        numpy.absolute(numpy.quantile(dataset[i + 1:j + 1], lower_q_threshold) - numpy.quantile(dataset[i + 1:j + 1], median_q_threshold)))\n",
    "    print('print here', temp)\n",
    "\n",
    "    for m in range(0, len(temp), steps):\n",
    "        q50array = []\n",
    "        q10array = []\n",
    "        q90array = []\n",
    "        q50 = numpy.quantile(temp[m:m + steps], median_q_threshold)\n",
    "        q50array.append(q50)\n",
    "        q50_array.append(q50array)\n",
    "\n",
    "        q90 = numpy.quantile(temp[m:m + steps], upper_q_threshold)\n",
    "        q90array.append(q90)\n",
    "        q90_array.append(q90array)\n",
    "\n",
    "        q10 = numpy.quantile(temp[m:m + steps], lower_q_threshold)\n",
    "        q10array.append(q10)\n",
    "        q10_array.append(q10array)\n",
    "\n",
    "    # print('stdarray1', std_array)\n",
    "    # std_array = numpy.array(std_array)\n",
    "    # print('stdarray2', std_array)\n",
    "    # std_array = numpy.reshape(std_array, (std_array.shape[0],std_array.shape[1], 1))\n",
    "    # print('stdarray3', std_array)\n",
    "    # print('avg_array',avg_array)\n",
    "    # avg_array = numpy.array(avg_array)\n",
    "    # avg_array = numpy.reshape(avg_array, (avg_array.shape[0],1, 1))\n",
    "    final_q50_array = []\n",
    "    final_q50_array.append(q50_array)\n",
    "    print('final_q10_array', final_q50_array)\n",
    "    final_q10_array = []\n",
    "    final_q10_array.append(q10_array)\n",
    "\n",
    "    final_q90_array = []\n",
    "    final_q90_array.append(q90_array)\n",
    "    # --- Predict q50 (Median) by averaging 3 models ---\n",
    "    pred_q50_1 = model_q50_1.predict(final_q50_array)\n",
    "    pred_q50_2 = model_q50_2.predict(final_q50_array)\n",
    "    pred_q50_3 = model_q50_3.predict(final_q50_array)\n",
    "    q50_predict = (pred_q50_1 + pred_q50_2 + pred_q50_3) / 3.0\n",
    "    print('q50_predict', q50_predict)\n",
    "\n",
    "    # --- Predict q10 (Lower) by averaging 3 models ---\n",
    "    pred_q10_1 = model_q10_1.predict(final_q10_array)\n",
    "    pred_q10_2 = model_q10_2.predict(final_q10_array)\n",
    "    pred_q10_3 = model_q10_3.predict(final_q10_array)\n",
    "    q10_predict = (pred_q10_1 + pred_q10_2 + pred_q10_3) / 3.0\n",
    "\n",
    "    # --- Predict q90 (Upper) by averaging 3 models ---\n",
    "    pred_q90_1 = model_q90_1.predict(final_q90_array)\n",
    "    pred_q90_2 = model_q90_2.predict(final_q90_array)\n",
    "    pred_q90_3 = model_q90_3.predict(final_q90_array)\n",
    "    q90_predict = (pred_q90_1 + pred_q90_2 + pred_q90_3) / 3.0\n",
    "# ...\n",
    "    if j+1 < len(dataset) :\n",
    "        iqr=q90_predict-q10_predict\n",
    "        ucl=q50_predict+.9*iqr\n",
    "        lcl=q50_predict-.9*iqr\n",
    "        diff=q50_predict-dataset[j+1]\n",
    "        if dataset[j+1]>ucl or dataset[j+1]<lcl:\n",
    "            anomalies.append(dataset[j+1])\n",
    "        #print('data',dataset[j+1],'diff',diff)\n",
    "\n",
    "        #comparison_dataset.append(dataset[j+1])\n",
    "        #dataset=numpy.delete(dataset,j+1)\n",
    "        #print('length',len(dataset))\n",
    "    #finalres_q10.append(q10_predict)\n",
    "    #finalres_q90.append(q90_predict)\n",
    "\n",
    "    j = j + 1\n",
    "    i = i + 1\n",
    "\n",
    "# print('finalres',finalres)\n",
    "'''prediction_array_q10 = []\n",
    "prediction_array_q90 = []'''\n",
    "anomalies_array=[]\n",
    "\n",
    "for h in range(len(anomalies)):\n",
    "    internal = anomalies[h]\n",
    "    internal_array = []\n",
    "    #internal_array.append(internal[0])\n",
    "    anomalies_array.append(internal)\n",
    "anomalies_array = scaler.inverse_transform(anomalies_array)\n",
    "#comparison_dataset=scaler.inverse_transform(comparison_dataset)\n",
    "#print(anomalies_array)\n",
    "print('anomaly_iden', anomalies_array)\n",
    "print('anomaly_iden size', len(anomalies_array))\n",
    "'''for itr in range(len(anomalies_array)):\n",
    "    print('data',comparison_dataset[itr],'diff',anomalies_array[itr])\n",
    "print('anomaly length',len(anomalies_array))'''\n",
    "'''ts_accumulate_another=[]\n",
    "for h in range(len(finalres_q10)):\n",
    "    internal = finalres_q10[h]\n",
    "    internal_q90 = finalres_q90[h]\n",
    "    prediction_array_q10.append(internal[0])\n",
    "    prediction_array_q90.append(internal_q90[0])\n",
    "for g in range(len(ts_accumulate)):\n",
    "    internal=[]\n",
    "    internal.append(ts_accumulate[g])\n",
    "    #internal_q90 = finalres_q90[h]\n",
    "    ts_accumulate_another.append(internal)\n",
    "finalres_q10 = scaler.inverse_transform(prediction_array_q10)\n",
    "finalres_q90 = scaler.inverse_transform(prediction_array_q90)\n",
    "#print('finalres', finalres_q10)'''\n",
    "'''trunc_finalres = []\n",
    "for g in range(len(finalres)):\n",
    "    trunc_finalres.append(finalres[g])'''\n",
    "'''ts = ts[look_back:]\n",
    "ts = scaler.inverse_transform(ts)\n",
    "ts_accumulate=scaler.inverse_transform(ts_accumulate_another)\n",
    "print('lenght', len(ts_accumulate), 'actual_quantile_interval', len(finalres_q10))'''\n",
    "'''ts_array = []\n",
    "for g in range(len(ts)):\n",
    "    ts_array.append(ts[g])'''\n",
    "'''finalres_q10_array=[]\n",
    "finalres_q90_array=[]\n",
    "for g in range(len(finalres_q10)-1):\n",
    "    finalres_q10_array.append(finalres_q10[g])\n",
    "    finalres_q90_array.append(finalres_q90[g])\n",
    "prediction_errors = []'''\n",
    "# for y in range(len(finalres_q10_array)):\n",
    "#    print('ts_accumulate',ts_accumulate[y], 'finalres_q10_array', finalres_q10_array[y], 'finalres_q90_array', finalres_q90_array[y])\n",
    "    #prediction_errors.append(numpy.absolute(actual_quantile_interval[y] - trunc_finalres[y]))\n",
    "# testScore = math.sqrt(mean_squared_error(ts, trunc_finalres))\n",
    "# print('Test Score: %.2f RMSE' % (testScore))\n",
    "'''anomalies = identify_anomaly(anomalies_array)\n",
    "print('anomaly_iden', anomalies)\n",
    "print('anomaly_iden size', len(anomalies))\n",
    "print(intersection(anomalies, anomaly_res))'''\n",
    "plt.figure(figsize=(12, 6))\n",
    "#plt.plot(ts, label=\"passengers\")\n",
    "plt.plot(dataset, label=\"dataset\")\n",
    "#plt.plot(anomalies_array, label=\"difference\")\n",
    "#plt.plot(newdata['ewa'], label=\"ewa\")\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "anomaly_res =[15,11]\n",
    "anomalies = numpy.unique(anomalies_array)\n",
    "print('anomaly_iden', anomalies)\n",
    "print('anomaly_iden size', len(anomalies))\n",
    "print('ani', anomaly_res)\n",
    "truep = (numpy.intersect1d(anomalies, numpy.array(anomaly_res)))\n",
    "print(\"TP:\")\n",
    "tp = len(truep)\n",
    "print(tp)\n",
    "print(\"FP:\")\n",
    "fp = (len(anomalies) - len(truep))\n",
    "print(fp)\n",
    "print(\"FN:\")\n",
    "fn = len(anomaly_res) - len(truep)\n",
    "print(fn)\n",
    "print(\"TN:\")\n",
    "print(1670-tp-fp-fn)\n",
    "#1670 - datapoints in yahoo8\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "print(\"Recall:\")\n",
    "print(recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
